{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e70aec9b-c2bd-49dd-89cb-92a36ff53841",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\hp/.cache\\torch\\hub\\intel-isl_MiDaS_master\n",
      "C:\\Users\\hp\\anaconda3\\envs\\midas_env\\lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "Using cache found in C:\\Users\\hp/.cache\\torch\\hub\\intel-isl_MiDaS_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 8 persons, 1 backpack, 7 chairs, 1 couch, 1 tv, 2 laptops, 183.8ms\n",
      "Speed: 4.9ms preprocess, 183.8ms inference, 6.6ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import ttk, filedialog, messagebox\n",
    "from PIL import Image, ImageTk\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pyttsx3\n",
    "import math\n",
    "\n",
    "from modules.yolo_module import YOLODetector\n",
    "from modules.midas_module import MiDaSDepth\n",
    "from modules.smolvlm_module import describe_image, describe_video\n",
    "\n",
    "# Compute left/center/right based on x coordinate\n",
    "def compute_direction(cx, width):\n",
    "    if cx < width * 0.33:\n",
    "        return \"left\"\n",
    "    elif cx > width * 0.66:\n",
    "        return \"right\"\n",
    "    else:\n",
    "        return \"center\"\n",
    "\n",
    "class VisionApp(tk.Tk):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.title(\"AI Vision Assistant - Guided Navigation\")\n",
    "        self.geometry(\"900x700\")\n",
    "\n",
    "        # UI\n",
    "        tk.Button(self, text=\"Load Image/Video\", command=self.load_file).pack(pady=10)\n",
    "        self.canvas = tk.Canvas(self, width=640, height=360, bg=\"black\")\n",
    "        self.canvas.pack()\n",
    "        self.txt_info = tk.Text(self, wrap=\"word\", height=12)\n",
    "        self.txt_info.pack(fill=\"both\", expand=True, padx=10, pady=10)\n",
    "        self.obj_dropdown = ttk.Combobox(self, state=\"readonly\")\n",
    "        self.obj_dropdown.pack(pady=5)\n",
    "        self.obj_dropdown.bind(\"<<ComboboxSelected>>\", self.on_object_select)\n",
    "\n",
    "        # Models & state\n",
    "        self.yolo = YOLODetector()\n",
    "        self.midas = MiDaSDepth()\n",
    "        self.tts = pyttsx3.init()\n",
    "        self.selected_image = None\n",
    "        self.depth_map = None\n",
    "        self.landmarks = []  # each is dict with label, bbox, dist, cx, cy\n",
    "        self.scene_overview = \"\"\n",
    "\n",
    "    def load_file(self):\n",
    "        path = filedialog.askopenfilename(filetypes=[(\"Media files\",\"*.jpg *.jpeg *.png *.mp4 *.avi\"),(\"All\",\"*.*\")])\n",
    "        if not path: return\n",
    "        self.canvas.delete(\"all\")\n",
    "        self.txt_info.delete(\"1.0\", tk.END)\n",
    "        self.process_image(path)\n",
    "\n",
    "    def process_image(self, path):\n",
    "        # display\n",
    "        img = Image.open(path).convert(\"RGB\").resize((640,360))\n",
    "        self.selected_image = img.copy()\n",
    "        self.photo = ImageTk.PhotoImage(img, master=self)\n",
    "        self.canvas.create_image(0,0,anchor='nw',image=self.photo)\n",
    "        \n",
    "        # YOLO + depth\n",
    "        raw = self.yolo.detect(img)\n",
    "        depth_arr, depth_vis = self.midas.estimate_depth(img)\n",
    "        self.depth_map = depth_arr\n",
    "        h,w = depth_arr.shape\n",
    "\n",
    "        # Scene overview from SMOL\n",
    "        self.scene_overview = describe_image(path)\n",
    "        self.txt_info.insert(tk.END, f\"Scene Overview:\\n{self.scene_overview}\\n\\n\")\n",
    "        self.tts.say(self.scene_overview)\n",
    "        self.tts.runAndWait()\n",
    "\n",
    "        # Build landmarks with geometry\n",
    "        self.landmarks = []\n",
    "        for idx,obj in enumerate(raw):\n",
    "            x1,y1,x2,y2 = map(int,obj['bbox'])\n",
    "            cx,cy = (x1+x2)//2,(y1+y2)//2\n",
    "            cx=np.clip(cx,0,w-1); cy=np.clip(cy,0,h-1)\n",
    "            dist=float(depth_arr[cy,cx])\n",
    "            self.landmarks.append({'label':obj['label'],'bbox':(x1,y1,x2,y2),'dist':round(dist,2),'cx':cx,'cy':cy})\n",
    "        # sort by distance\n",
    "        self.landmarks.sort(key=lambda o:o['dist'])\n",
    "\n",
    "        # list landmarks\n",
    "        self.txt_info.insert(tk.END,\"Landmarks (nearest first):\\n\")\n",
    "        for i, lm in enumerate(self.landmarks):\n",
    "            self.txt_info.insert(tk.END,f\"{i}. {lm['label']} at {lm['dist']} m\\n\")\n",
    "\n",
    "        # dropdown\n",
    "        vals=[f\"{lm['label']} ({i})\" for i,lm in enumerate(self.landmarks)]\n",
    "        self.obj_dropdown['values']=vals\n",
    "        if vals: self.obj_dropdown.set(vals[0])\n",
    "\n",
    "        # show depth map\n",
    "        depth_win=tk.Toplevel(self); depth_win.title(\"Depth Map\")\n",
    "        dv_img=depth_vis.resize((320,180)) if isinstance(depth_vis,Image.Image) else Image.fromarray((depth_vis*255).astype(np.uint8)).resize((320,180))\n",
    "        ph=ImageTk.PhotoImage(dv_img,master=depth_win); tk.Label(depth_win,image=ph).pack(); depth_win.image=ph\n",
    "\n",
    "    def on_object_select(self, event):\n",
    "        idx = self.obj_dropdown.current()\n",
    "        if idx < 0 or idx >= len(self.landmarks): return\n",
    "\n",
    "        user_x, user_y = 320, 360  # bottom-center\n",
    "        current_angle = 0.0\n",
    "        steps = []\n",
    "        target = self.landmarks[idx]\n",
    "\n",
    "        # Helper: detect blocking obstacles\n",
    "        def is_blocking_path(obs, target):\n",
    "            dx1, dx2 = sorted([obs['cx'], target['cx']])\n",
    "            dy1, dy2 = sorted([obs['cy'], target['cy']])\n",
    "            return (\n",
    "                dx1 - 30 <= user_x <= dx2 + 30 and\n",
    "                obs['cy'] >= target['cy'] and\n",
    "                obs['dist'] < target['dist'] and\n",
    "                abs(obs['cx'] - target['cx']) < 100\n",
    "            )\n",
    "\n",
    "        # Identify blocking obstacles\n",
    "        blocking = [lm for i, lm in enumerate(self.landmarks) if i != idx and is_blocking_path(lm, target)]\n",
    "\n",
    "        if blocking:\n",
    "            for obs in blocking:\n",
    "                direction = compute_direction(obs['cx'], 640)\n",
    "                steps.append(f\"Obstacle detected: {obs['label']} ahead on the {direction}.\")\n",
    "                if direction == \"left\":\n",
    "                    steps.append(\"Turn 30° right to avoid the obstacle.\")\n",
    "                    current_angle += 30\n",
    "                elif direction == \"right\":\n",
    "                    steps.append(\"Turn 30° left to avoid the obstacle.\")\n",
    "                    current_angle -= 30\n",
    "                else:\n",
    "                    steps.append(\"Step slightly to your left to bypass the obstacle.\")\n",
    "        else:\n",
    "            # Direct path to target\n",
    "            dx = target['cx'] - user_x\n",
    "            dy = user_y - target['cy']\n",
    "            angle = math.degrees(math.atan2(dx, dy))\n",
    "            turn = angle - current_angle\n",
    "            if abs(turn) > 10:\n",
    "                direction = 'right' if turn > 0 else 'left'\n",
    "                steps.append(f\"Turn {abs(int(turn))}° {direction}\")\n",
    "            steps.append(f\"Walk forward {target['dist']:.1f} meters to reach the {target['label']}.\")\n",
    "\n",
    "        # TTS and display\n",
    "        self.txt_info.insert(tk.END, \"\\nNavigation Steps:\\n\")\n",
    "        for s in steps:\n",
    "            self.txt_info.insert(tk.END, s + \"\\n\")\n",
    "            self.tts.say(s)\n",
    "        self.tts.runAndWait()\n",
    "\n",
    "        # Highlight selected object\n",
    "        x1, y1, x2, y2 = target['bbox']\n",
    "        arr = np.array(self.selected_image)\n",
    "        cv2.rectangle(arr, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
    "        disp = Image.fromarray(arr).resize((640, 360))\n",
    "        self.photo = ImageTk.PhotoImage(disp, master=self)\n",
    "        self.canvas.create_image(0, 0, anchor='nw', image=self.photo)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app = VisionApp()\n",
    "    app.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f8ef402-5963-40e0-92a6-d4b07058c4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\hp/.cache\\torch\\hub\\intel-isl_MiDaS_master\n",
      "Using cache found in C:\\Users\\hp/.cache\\torch\\hub\\intel-isl_MiDaS_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 (no detections), 168.6ms\n",
      "Speed: 3.5ms preprocess, 168.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 laptop, 1 keyboard, 177.9ms\n",
      "Speed: 0.8ms preprocess, 177.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 laptop, 169.0ms\n",
      "Speed: 2.2ms preprocess, 169.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 laptop, 182.4ms\n",
      "Speed: 1.5ms preprocess, 182.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 laptop, 172.0ms\n",
      "Speed: 2.3ms preprocess, 172.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 mouse, 172.7ms\n",
      "Speed: 1.8ms preprocess, 172.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 mouse, 166.5ms\n",
      "Speed: 1.6ms preprocess, 166.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 mouse, 162.0ms\n",
      "Speed: 1.8ms preprocess, 162.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 mouse, 168.0ms\n",
      "Speed: 1.7ms preprocess, 168.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 191.6ms\n",
      "Speed: 1.9ms preprocess, 191.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 mouse, 164.4ms\n",
      "Speed: 1.8ms preprocess, 164.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 dog, 1 mouse, 1 book, 171.9ms\n",
      "Speed: 1.9ms preprocess, 171.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 175.6ms\n",
      "Speed: 0.8ms preprocess, 175.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 205.3ms\n",
      "Speed: 0.8ms preprocess, 205.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 171.5ms\n",
      "Speed: 1.9ms preprocess, 171.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 163.3ms\n",
      "Speed: 1.1ms preprocess, 163.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 172.4ms\n",
      "Speed: 2.4ms preprocess, 172.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 169.1ms\n",
      "Speed: 2.0ms preprocess, 169.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 183.5ms\n",
      "Speed: 1.6ms preprocess, 183.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 165.7ms\n",
      "Speed: 2.4ms preprocess, 165.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 166.4ms\n",
      "Speed: 1.9ms preprocess, 166.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 171.4ms\n",
      "Speed: 1.7ms preprocess, 171.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 remote, 171.4ms\n",
      "Speed: 1.0ms preprocess, 171.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 1 keyboard, 166.8ms\n",
      "Speed: 1.2ms preprocess, 166.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 170.9ms\n",
      "Speed: 2.6ms preprocess, 170.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 161.9ms\n",
      "Speed: 0.8ms preprocess, 161.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 176.7ms\n",
      "Speed: 2.1ms preprocess, 176.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 181.1ms\n",
      "Speed: 2.3ms preprocess, 181.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 168.7ms\n",
      "Speed: 1.2ms preprocess, 168.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 177.0ms\n",
      "Speed: 2.4ms preprocess, 177.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 189.3ms\n",
      "Speed: 1.1ms preprocess, 189.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 172.8ms\n",
      "Speed: 1.4ms preprocess, 172.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 laptop, 165.1ms\n",
      "Speed: 1.5ms preprocess, 165.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 laptop, 215.0ms\n",
      "Speed: 0.8ms preprocess, 215.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 tv, 2 laptops, 1 keyboard, 168.6ms\n",
      "Speed: 1.1ms preprocess, 168.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 chair, 165.3ms\n",
      "Speed: 0.8ms preprocess, 165.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 tv, 1 laptop, 1 keyboard, 178.3ms\n",
      "Speed: 1.6ms preprocess, 178.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cat, 2 chairs, 1 tv, 184.8ms\n",
      "Speed: 0.8ms preprocess, 184.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 chairs, 1 dining table, 197.4ms\n",
      "Speed: 1.1ms preprocess, 197.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 165.3ms\n",
      "Speed: 1.0ms preprocess, 165.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 1 backpack, 6 chairs, 1 tv, 2 laptops, 171.1ms\n",
      "Speed: 1.6ms preprocess, 171.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5 cars, 149.4ms\n",
      "Speed: 1.6ms preprocess, 149.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import ttk, filedialog, messagebox\n",
    "from PIL import Image, ImageTk\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pyttsx3\n",
    "import math\n",
    "import heapq\n",
    "\n",
    "from modules.yolo_module import YOLODetector\n",
    "from modules.midas_module import MiDaSDepth\n",
    "from modules.smolvlm_module import describe_image, describe_video\n",
    "\n",
    "class GridCell:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.blocked = False\n",
    "        self.parent = None\n",
    "        self.g = 0\n",
    "        self.h = 0\n",
    "        self.f = 0\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.f < other.f\n",
    "\n",
    "class NavigationGrid:\n",
    "    def __init__(self, rows, cols, cell_size):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.cell_size = cell_size\n",
    "        self.grid = [[GridCell(c, r) for c in range(cols)] for r in range(rows)]\n",
    "        self.start = (rows-1, cols//2)\n",
    "        self.goal = None\n",
    "        \n",
    "    def update_obstacles(self, depth_map, landmarks, threshold=2.0):\n",
    "        h, w = depth_map.shape\n",
    "        for r in range(self.rows):\n",
    "            for c in range(self.cols):\n",
    "                y1 = int((r * h) / self.rows)\n",
    "                y2 = int(((r+1) * h) / self.rows)\n",
    "                x1 = int((c * w) / self.cols)\n",
    "                x2 = int(((c+1) * w) / self.cols)\n",
    "                cell_depth = np.mean(depth_map[y1:y2, x1:x2])\n",
    "                \n",
    "                landmark_in_cell = any(\n",
    "                    x1 <= lm['cx'] <= x2 and y1 <= lm['cy'] <= y2\n",
    "                    for lm in landmarks\n",
    "                )\n",
    "                \n",
    "                self.grid[r][c].blocked = cell_depth < threshold or landmark_in_cell\n",
    "\n",
    "    def find_path_a_star(self):\n",
    "        if not self.goal:\n",
    "            return []\n",
    "\n",
    "        open_list = []\n",
    "        start_cell = self.grid[self.start[0]][self.start[1]]\n",
    "        goal_cell = self.grid[self.goal[0]][self.goal[1]]\n",
    "        \n",
    "        heapq.heappush(open_list, start_cell)\n",
    "        \n",
    "        while open_list:\n",
    "            current = heapq.heappop(open_list)\n",
    "            \n",
    "            if current == goal_cell:\n",
    "                path = []\n",
    "                while current:\n",
    "                    path.append((current.x, current.y))\n",
    "                    current = current.parent\n",
    "                return path[::-1]\n",
    "            \n",
    "            for dr, dc in [(-1,0),(1,0),(0,-1),(0,1), (-1,-1),(-1,1),(1,-1),(1,1)]:\n",
    "                nr, nc = current.y + dr, current.x + dc\n",
    "                if 0 <= nr < self.rows and 0 <= nc < self.cols:\n",
    "                    neighbor = self.grid[nr][nc]\n",
    "                    if neighbor.blocked:\n",
    "                        continue\n",
    "                    \n",
    "                    tentative_g = current.g + math.hypot(dr, dc)\n",
    "                    if tentative_g < neighbor.g or not neighbor.parent:\n",
    "                        neighbor.parent = current\n",
    "                        neighbor.g = tentative_g\n",
    "                        neighbor.h = math.hypot(nc - goal_cell.x, nr - goal_cell.y)\n",
    "                        neighbor.f = neighbor.g + neighbor.h\n",
    "                        heapq.heappush(open_list, neighbor)\n",
    "        \n",
    "        return []\n",
    "\n",
    "class VisionApp(tk.Tk):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.title(\"AI Vision Assistant - Guided Navigation\")\n",
    "        self.geometry(\"1200x800\")\n",
    "        \n",
    "        # Initialize modules\n",
    "        self.yolo = YOLODetector()\n",
    "        self.midas = MiDaSDepth()\n",
    "        self.tts = pyttsx3.init()\n",
    "        \n",
    "        # State variables\n",
    "        self.landmarks = []\n",
    "        self.current_path = []\n",
    "        self.camera_active = False\n",
    "        self.cap = None\n",
    "        self.nav_grid = NavigationGrid(20, 20, 32)\n",
    "        self.update_interval = 100\n",
    "        \n",
    "        # UI Setup\n",
    "        self.setup_ui()\n",
    "\n",
    "    def setup_ui(self):\n",
    "        control_frame = tk.Frame(self)\n",
    "        control_frame.pack(pady=10)\n",
    "        \n",
    "        tk.Button(control_frame, text=\"Load Image/Video\", \n",
    "                command=self.load_file).pack(side=tk.LEFT, padx=5)\n",
    "        self.cam_btn = tk.Button(control_frame, text=\"Start Camera\", \n",
    "                               command=self.toggle_camera)\n",
    "        self.cam_btn.pack(side=tk.LEFT, padx=5)\n",
    "        tk.Button(control_frame, text=\"Clear\", \n",
    "                command=self.clear).pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        self.obj_dropdown = ttk.Combobox(control_frame, state=\"readonly\")\n",
    "        self.obj_dropdown.pack(side=tk.LEFT, padx=5)\n",
    "        self.obj_dropdown.bind(\"<<ComboboxSelected>>\", self.on_object_select)\n",
    "        \n",
    "        self.canvas = tk.Canvas(self, width=640, height=360, bg=\"black\")\n",
    "        self.canvas.pack(pady=10)\n",
    "        self.depth_canvas = tk.Canvas(self, width=320, height=180, bg=\"black\")\n",
    "        self.depth_canvas.pack()\n",
    "        \n",
    "        self.txt_info = tk.Text(self, wrap=\"word\", height=15)\n",
    "        self.txt_info.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)\n",
    "        \n",
    "        # Map window\n",
    "        self.map_window = tk.Toplevel(self)\n",
    "        self.map_window.title(\"Navigation Map\")\n",
    "        self.map_canvas = tk.Canvas(self.map_window, width=400, height=400, bg=\"white\")\n",
    "        self.map_canvas.pack()\n",
    "        self.draw_grid()\n",
    "\n",
    "    def draw_grid(self):\n",
    "        cell_size = 20\n",
    "        for r in range(self.nav_grid.rows):\n",
    "            for c in range(self.nav_grid.cols):\n",
    "                x1 = c * cell_size\n",
    "                y1 = r * cell_size\n",
    "                x2 = x1 + cell_size\n",
    "                y2 = y1 + cell_size\n",
    "                self.map_canvas.create_rectangle(x1, y1, x2, y2, outline=\"gray\")\n",
    "\n",
    "    def update_map(self):\n",
    "        cell_size = 20\n",
    "        self.map_canvas.delete(\"all\")\n",
    "        \n",
    "        for r in range(self.nav_grid.rows):\n",
    "            for c in range(self.nav_grid.cols):\n",
    "                x1 = c * cell_size\n",
    "                y1 = r * cell_size\n",
    "                x2 = x1 + cell_size\n",
    "                y2 = y1 + cell_size\n",
    "                color = \"red\" if self.nav_grid.grid[r][c].blocked else \"white\"\n",
    "                self.map_canvas.create_rectangle(x1, y1, x2, y2, fill=color, outline=\"gray\")\n",
    "        \n",
    "        if self.current_path:\n",
    "            for (c, r) in self.current_path:\n",
    "                x = c * cell_size + cell_size//2\n",
    "                y = r * cell_size + cell_size//2\n",
    "                self.map_canvas.create_oval(x-3, y-3, x+3, y+3, fill=\"blue\")\n",
    "        \n",
    "        start_r, start_c = self.nav_grid.start\n",
    "        self.map_canvas.create_oval(start_c*cell_size+5, start_r*cell_size+5,\n",
    "                                  (start_c+1)*cell_size-5, (start_r+1)*cell_size-5,\n",
    "                                  fill=\"green\")\n",
    "        if self.nav_grid.goal:\n",
    "            goal_r, goal_c = self.nav_grid.goal\n",
    "            self.map_canvas.create_oval(goal_c*cell_size+5, goal_r*cell_size+5,\n",
    "                                      (goal_c+1)*cell_size-5, (goal_r+1)*cell_size-5,\n",
    "                                      fill=\"yellow\")\n",
    "\n",
    "    def load_file(self):\n",
    "        path = filedialog.askopenfilename(filetypes=[\n",
    "            (\"Media files\", \"*.jpg *.jpeg *.png *.mp4 *.avi\"),\n",
    "            (\"All files\", \"*.*\")\n",
    "        ])\n",
    "        if path:\n",
    "            self.process_file(path)\n",
    "\n",
    "    def process_file(self, path):\n",
    "        self.clear()\n",
    "        try:\n",
    "            if path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                img = Image.open(path).convert(\"RGB\").resize((640, 360))\n",
    "                self.process_frame(img, is_live=False)\n",
    "                # Get scene description\n",
    "                description = describe_image(path)\n",
    "                self.txt_info.insert(tk.END, f\"Scene Description:\\n{description}\\n\")\n",
    "                self.tts.say(description)\n",
    "                self.tts.runAndWait()\n",
    "            elif path.lower().endswith(('.mp4', '.avi')):\n",
    "                self.process_video(path)\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Failed to process file: {str(e)}\")\n",
    "\n",
    "    def process_video(self, path):\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            pil_img = Image.fromarray(frame)\n",
    "            self.process_frame(pil_img, is_live=False)\n",
    "            self.update_idletasks()\n",
    "            self.after(50)\n",
    "        cap.release()\n",
    "\n",
    "    def toggle_camera(self):\n",
    "        if self.camera_active:\n",
    "            self.stop_camera()\n",
    "        else:\n",
    "            self.start_camera()\n",
    "\n",
    "    def start_camera(self):\n",
    "        self.cap = cv2.VideoCapture(0)\n",
    "        if not self.cap.isOpened():\n",
    "            messagebox.showerror(\"Error\", \"Could not open camera\")\n",
    "            return\n",
    "        self.camera_active = True\n",
    "        self.cam_btn.config(text=\"Stop Camera\")\n",
    "        self.update_camera()\n",
    "\n",
    "    def stop_camera(self):\n",
    "        self.camera_active = False\n",
    "        if self.cap:\n",
    "            self.cap.release()\n",
    "        self.cam_btn.config(text=\"Start Camera\")\n",
    "\n",
    "    def update_camera(self):\n",
    "        if self.camera_active:\n",
    "            ret, frame = self.cap.read()\n",
    "            if ret:\n",
    "                # Convert OpenCV frame to PIL Image\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                pil_img = Image.fromarray(frame)\n",
    "                self.process_frame(pil_img, is_live=True)\n",
    "            self.after(self.update_interval, self.update_camera)\n",
    "        \n",
    "    def process_frame(self, img, is_live=True):\n",
    "        pil_img = img.resize((640, 360))\n",
    "    \n",
    "        # Convert to numpy array for YOLO\n",
    "        img_np = np.array(pil_img)\n",
    "        \n",
    "        # Process through models\n",
    "        detections = self.yolo.detect(img_np)\n",
    "        depth_arr, depth_vis = self.midas.estimate_depth(pil_img)\n",
    "        \n",
    "        # Update landmarks and navigation\n",
    "        self.landmarks = self.parse_landmarks(detections, depth_arr)\n",
    "        self.nav_grid.update_obstacles(depth_arr, self.landmarks)\n",
    "        if self.nav_grid.goal:\n",
    "            self.current_path = self.nav_grid.find_path_a_star()\n",
    "        \n",
    "        # Update displays\n",
    "        self.update_image_display(img_np, detections)\n",
    "        self.update_depth_display(depth_vis)\n",
    "        self.update_map()\n",
    "        \n",
    "        # Update dropdown for static images\n",
    "        if not is_live and self.landmarks:\n",
    "            values = [f\"{lm['label']} ({i})\" for i, lm in enumerate(self.landmarks)]\n",
    "            self.obj_dropdown['values'] = values\n",
    "            self.obj_dropdown.set(values[0])\n",
    "\n",
    "    def parse_landmarks(self, detections, depth_map):\n",
    "        landmarks = []\n",
    "        h, w = depth_map.shape\n",
    "        for obj in detections:\n",
    "            x1, y1, x2, y2 = map(int, obj['bbox'])\n",
    "            cx = (x1 + x2) // 2\n",
    "            cy = (y1 + y2) // 2\n",
    "            cx = np.clip(cx, 0, w-1)\n",
    "            cy = np.clip(cy, 0, h-1)\n",
    "            dist = float(depth_map[cy, cx])\n",
    "            landmarks.append({\n",
    "                'label': obj['label'],\n",
    "                'bbox': (x1, y1, x2, y2),\n",
    "                'dist': round(dist, 2),\n",
    "                'cx': cx,\n",
    "                'cy': cy\n",
    "            })\n",
    "        return sorted(landmarks, key=lambda x: x['dist'])\n",
    "\n",
    "    def update_image_display(self, img_np, detections):\n",
    "        # Draw bounding boxes\n",
    "        for obj in detections:\n",
    "            x1, y1, x2, y2 = obj['bbox']\n",
    "            cv2.rectangle(img_np, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(img_np, obj['label'], (x1, y1-10),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 2)\n",
    "        \n",
    "        # Convert to PhotoImage\n",
    "        img = Image.fromarray(img_np)\n",
    "        self.photo = ImageTk.PhotoImage(img)\n",
    "        self.canvas.create_image(0, 0, image=self.photo, anchor=tk.NW)\n",
    "\n",
    "    def update_depth_display(self, depth_vis):\n",
    "        if isinstance(depth_vis, np.ndarray):\n",
    "            depth_img = Image.fromarray((depth_vis * 255).astype(np.uint8)).resize((320, 180))\n",
    "        else:\n",
    "            depth_img = depth_vis.resize((320, 180))\n",
    "        depth_photo = ImageTk.PhotoImage(depth_img)\n",
    "        self.depth_canvas.create_image(0, 0, image=depth_photo, anchor=tk.NW)\n",
    "        self.depth_canvas.image = depth_photo\n",
    "\n",
    "    def on_object_select(self, event):\n",
    "        idx = self.obj_dropdown.current()\n",
    "        if 0 <= idx < len(self.landmarks):\n",
    "            target = self.landmarks[idx]\n",
    "            self.nav_grid.goal = self.pixel_to_grid(target['cx'], target['cy'])\n",
    "            self.current_path = self.nav_grid.find_path_a_star()\n",
    "            self.update_map()\n",
    "            self.generate_navigation_instructions()\n",
    "\n",
    "    def pixel_to_grid(self, x, y):\n",
    "        grid_x = int((x / 640) * self.nav_grid.cols)\n",
    "        grid_y = int((y / 360) * self.nav_grid.rows)\n",
    "        return (grid_y, grid_x)\n",
    "\n",
    "    def generate_navigation_instructions(self):\n",
    "        if not self.current_path:\n",
    "            self.txt_info.insert(tk.END, \"No valid path found!\\n\")\n",
    "            return\n",
    "\n",
    "        instructions = []\n",
    "        prev = self.nav_grid.start\n",
    "        for cell in self.current_path[1:]:\n",
    "            dr = cell[0] - prev[0]\n",
    "            dc = cell[1] - prev[1]\n",
    "            \n",
    "            if dr < 0:\n",
    "                instructions.append(\"Move forward\")\n",
    "            elif dr > 0:\n",
    "                instructions.append(\"Move backward\")\n",
    "                \n",
    "            if dc < 0:\n",
    "                instructions.append(\"Turn slightly left\")\n",
    "            elif dc > 0:\n",
    "                instructions.append(\"Turn slightly right\")\n",
    "            \n",
    "            prev = cell\n",
    "\n",
    "        self.txt_info.delete(1.0, tk.END)\n",
    "        self.txt_info.insert(tk.END, \"\\n\".join(instructions))\n",
    "        for step in instructions:\n",
    "            self.tts.say(step)\n",
    "        self.tts.runAndWait()\n",
    "\n",
    "    def clear(self):\n",
    "        self.stop_camera()\n",
    "        self.canvas.delete(\"all\")\n",
    "        self.depth_canvas.delete(\"all\")\n",
    "        self.txt_info.delete(1.0, tk.END)\n",
    "        self.landmarks = []\n",
    "        self.current_path = []\n",
    "        self.nav_grid = NavigationGrid(20, 20, 32)\n",
    "        self.update_map()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app = VisionApp()\n",
    "    app.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ab0939-f76e-415b-b1e0-8ecdcd70dc21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
